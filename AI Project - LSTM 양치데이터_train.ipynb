{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "actions = ['uf_outside', 'uf_inside',\n",
    "           'ur_outside', 'ur_inside', 'ur_above',\n",
    "           'ul_outside', 'ul_inside', 'ul_above',\n",
    "           'lf_outside', 'lf_inside',\n",
    "           'lr_outside', 'lr_inside', 'lr_above',\n",
    "           'll_outside', 'll_inside', 'll_avobe']\n",
    "\n",
    "data = np.concatenate([\n",
    "    np.load('dataset/seq_lf_inside_은정30.npy'),\n",
    "    np.load('dataset/seq_lf_outside_은정30.npy'),\n",
    "    np.load('dataset/seq_ll_avobe_은정30.npy'),\n",
    "    np.load('dataset/seq_ll_inside_은정30.npy'),\n",
    "    np.load('dataset/seq_ll_outside_은정30.npy'),\n",
    "    np.load('dataset/seq_lr_above_은정30.npy'),\n",
    "    np.load('dataset/seq_lr_inside_은정30.npy'),\n",
    "    np.load('dataset/seq_lr_outside_은정30.npy'),\n",
    "    np.load('dataset/seq_uf_inside_은정30.npy'),\n",
    "    np.load('dataset/seq_uf_outside_은정30.npy'),\n",
    "    np.load('dataset/seq_ul_above_은정30.npy'),\n",
    "    np.load('dataset/seq_ul_inside_은정30.npy'),\n",
    "    np.load('dataset/seq_ul_outside_은정30.npy'),\n",
    "    np.load('dataset/seq_ur_above_은정30.npy'),\n",
    "    np.load('dataset/seq_ur_inside_은정30.npy'),\n",
    "    np.load('dataset/seq_ur_outside_은정30.npy'),\n",
    "    #np.load('dataset/seq_lf_inside_보현30.npy'),\n",
    "    np.load('dataset/seq_lf_outside_보현30.npy'),\n",
    "    np.load('dataset/seq_ll_avobe_보현30.npy'),\n",
    "    np.load('dataset/seq_ll_inside_보현30.npy'),\n",
    "    np.load('dataset/seq_ll_outside_보현30.npy'),\n",
    "    np.load('dataset/seq_lr_above_보현30.npy'),\n",
    "    np.load('dataset/seq_lr_inside_보현30.npy'),\n",
    "    np.load('dataset/seq_lr_outside_보현30.npy'),\n",
    "    np.load('dataset/seq_uf_inside_보현30.npy'),\n",
    "    np.load('dataset/seq_uf_outside_보현30.npy'),\n",
    "    np.load('dataset/seq_ul_above_보현30.npy'),\n",
    "    np.load('dataset/seq_ul_inside_보현30.npy'),\n",
    "    np.load('dataset/seq_ul_outside_보현30.npy'),\n",
    "    np.load('dataset/seq_ur_above_보현30.npy'),\n",
    "    np.load('dataset/seq_ur_inside_보현30.npy'),\n",
    "    np.load('dataset/seq_ur_outside_보현30.npy'),\n",
    "    np.load('dataset/seq_lf_inside_은정302.npy'),\n",
    "    np.load('dataset/seq_lf_outside_은정302.npy'),\n",
    "    np.load('dataset/seq_ll_avobe_은정302.npy'),\n",
    "    np.load('dataset/seq_ll_inside_은정302.npy'),\n",
    "    np.load('dataset/seq_ll_outside_은정302.npy'),\n",
    "    np.load('dataset/seq_lr_above_은정302.npy'),\n",
    "    np.load('dataset/seq_lr_inside_은정302.npy'),\n",
    "    np.load('dataset/seq_lr_outside_은정302.npy'),\n",
    "    np.load('dataset/seq_uf_inside_은정302.npy'),\n",
    "    np.load('dataset/seq_uf_outside_은정302.npy'),\n",
    "    np.load('dataset/seq_ul_above_은정302.npy'),\n",
    "    np.load('dataset/seq_ul_inside_은정302.npy'),\n",
    "    np.load('dataset/seq_ul_outside_은정302.npy'),\n",
    "    np.load('dataset/seq_ur_above_은정302.npy'),\n",
    "    #np.load('dataset/seq_ur_inside_은정302.npy'),\n",
    "    np.load('dataset/seq_ur_outside_은정302.npy'),\n",
    "    np.load('dataset/seq_lf_inside_채환30.npy'),\n",
    "    np.load('dataset/seq_lf_outside_채환30.npy'),\n",
    "    np.load('dataset/seq_ll_avobe_채환30.npy'),\n",
    "    np.load('dataset/seq_ll_inside_채환30.npy'),\n",
    "    np.load('dataset/seq_ll_outside_채환30.npy'),\n",
    "    np.load('dataset/seq_lr_above_채환30.npy'),\n",
    "    np.load('dataset/seq_lr_inside_채환30.npy'),\n",
    "    np.load('dataset/seq_lr_outside_채환30.npy'),\n",
    "    np.load('dataset/seq_uf_inside_채환30.npy'),\n",
    "    np.load('dataset/seq_uf_outside_채환30.npy'),\n",
    "    np.load('dataset/seq_ul_above_채환30.npy'),\n",
    "    np.load('dataset/seq_ul_inside_채환30.npy'),\n",
    "    np.load('dataset/seq_ul_outside_채환30.npy'),\n",
    "    np.load('dataset/seq_ur_above_채환30.npy'),\n",
    "    np.load('dataset/seq_ur_inside_채환30.npy'),\n",
    "    np.load('dataset/seq_ur_outside_채환30.npy'),\n",
    "], axis=0)\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(421, 30, 106)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions = ['ll_above', 'uf_outside']\n",
    "\n",
    "data = np.concatenate([\n",
    "    # np.load('dataset/합친거/seq_lf_inside_은정30.npy'),\n",
    "    # np.load('dataset/합친거/seq_lf_outside_은정30.npy'),\n",
    "    # np.load('dataset/합친거/seq_ll_avobe_은정30.npy'),\n",
    "    # np.load('dataset/합친거/seq_ll_inside_은정30.npy'),\n",
    "    # np.load('dataset/합친거/seq_ll_outside_은정30.npy'),\n",
    "    # np.load('dataset/합친거/seq_lr_above_은정30.npy'),\n",
    "    # np.load('dataset/합친거/seq_lr_inside_은정30.npy'),\n",
    "    # np.load('dataset/합친거/seq_lr_outside_은정30.npy'),\n",
    "    # np.load('dataset/합친거/seq_uf_inside_은정30.npy'),\n",
    "    # np.load('dataset/합친거/seq_uf_outside_은정30.npy'),\n",
    "    # np.load('dataset/합친거/seq_ul_above_은정30.npy'),\n",
    "    # np.load('dataset/합친거/seq_ul_inside_은정30.npy'),\n",
    "    # np.load('dataset/합친거/seq_ul_outside_은정30.npy'),\n",
    "    # np.load('dataset/합친거/seq_ur_above_은정30.npy'),\n",
    "    # np.load('dataset/합친거/seq_ur_inside_은정30.npy')\n",
    "], axis=0)\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(821, 30, 105)\n",
      "(821,)\n"
     ]
    }
   ],
   "source": [
    "x_data = data[:, :, :-1]\n",
    "labels = data[:, 0, -1]\n",
    "\n",
    "print(x_data.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 23:36:06.449038: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-18 23:36:06.614534: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-18 23:36:06.619797: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-12-18 23:36:06.619817: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-12-18 23:36:06.649169: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-18 23:36:07.240276: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-12-18 23:36:07.240332: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-12-18 23:36:07.240339: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(821, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_data = to_categorical(labels, num_classes=len(actions))\n",
    "y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(738, 30, 105) (738, 2)\n",
      "(83, 30, 105) (83, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_data = x_data.astype(np.float32)\n",
    "y_data = y_data.astype(np.float32)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.1, random_state=2021)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 64)                43520     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 45,666\n",
      "Trainable params: 45,666\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 23:36:08.372676: E tensorflow/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-12-18 23:36:08.372715: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: piai-Precision-7920-Tower\n",
      "2023-12-18 23:36:08.372723: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: piai-Precision-7920-Tower\n",
      "2023-12-18 23:36:08.372860: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 535.54.3\n",
      "2023-12-18 23:36:08.372882: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 535.54.3\n",
      "2023-12-18 23:36:08.372888: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 535.54.3\n",
      "2023-12-18 23:36:08.373364: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(64, activation='relu', input_shape=x_train.shape[1:3]),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(len(actions), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 53.9085 - acc: 0.5707\n",
      "Epoch 1: val_acc improved from -inf to 0.71084, saving model to models/model.h5\n",
      "24/24 [==============================] - 2s 42ms/step - loss: 53.7624 - acc: 0.5718 - val_loss: 54.1854 - val_acc: 0.7108 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 47.8901 - acc: 0.6793\n",
      "Epoch 2: val_acc improved from 0.71084 to 0.73494, saving model to models/model.h5\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 47.7891 - acc: 0.6789 - val_loss: 14.0529 - val_acc: 0.7349 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 9.8243 - acc: 0.8696 \n",
      "Epoch 3: val_acc improved from 0.73494 to 0.96386, saving model to models/model.h5\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 10.0282 - acc: 0.8686 - val_loss: 2.2162 - val_acc: 0.9639 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 3.9687 - acc: 0.9497\n",
      "Epoch 4: val_acc improved from 0.96386 to 0.98795, saving model to models/model.h5\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 3.9580 - acc: 0.9499 - val_loss: 0.7608 - val_acc: 0.9880 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 3.4909 - acc: 0.9552\n",
      "Epoch 5: val_acc did not improve from 0.98795\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 3.4814 - acc: 0.9553 - val_loss: 2.3637 - val_acc: 0.9639 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 4.2844 - acc: 0.9524\n",
      "Epoch 6: val_acc did not improve from 0.98795\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 4.2728 - acc: 0.9526 - val_loss: 3.8405 - val_acc: 0.9639 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 16.1649 - acc: 0.8628\n",
      "Epoch 7: val_acc did not improve from 0.98795\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 16.1211 - acc: 0.8631 - val_loss: 11.7010 - val_acc: 0.8795 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 38.5847 - acc: 0.8546\n",
      "Epoch 8: val_acc did not improve from 0.98795\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 38.4801 - acc: 0.8550 - val_loss: 9.9379 - val_acc: 0.9398 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 27.9061 - acc: 0.8954\n",
      "Epoch 9: val_acc did not improve from 0.98795\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 27.8305 - acc: 0.8957 - val_loss: 48.6478 - val_acc: 0.8193 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 97.4223 - acc: 0.6766 \n",
      "Epoch 10: val_acc did not improve from 0.98795\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 97.2190 - acc: 0.6762 - val_loss: 68.0322 - val_acc: 0.8193 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 27.8491 - acc: 0.8777\n",
      "Epoch 11: val_acc did not improve from 0.98795\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 27.7737 - acc: 0.8780 - val_loss: 5.9478 - val_acc: 0.9157 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 2.9336 - acc: 0.9728\n",
      "Epoch 12: val_acc did not improve from 0.98795\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 2.9256 - acc: 0.9729 - val_loss: 73.1054 - val_acc: 0.7108 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 114.8379 - acc: 0.7432\n",
      "Epoch 13: val_acc did not improve from 0.98795\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 114.5267 - acc: 0.7439 - val_loss: 77.2388 - val_acc: 0.5542 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 49.7438 - acc: 0.6182\n",
      "Epoch 14: val_acc did not improve from 0.98795\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 49.6091 - acc: 0.6192 - val_loss: 43.4620 - val_acc: 0.6145 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 82.3454 - acc: 0.6277\n",
      "Epoch 15: val_acc did not improve from 0.98795\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 82.3753 - acc: 0.6274 - val_loss: 253.2267 - val_acc: 0.4096 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 379.7657 - acc: 0.4633\n",
      "Epoch 16: val_acc did not improve from 0.98795\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 378.7365 - acc: 0.4648 - val_loss: 509.6469 - val_acc: 0.6506 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 214.2077 - acc: 0.7242\n",
      "Epoch 17: val_acc did not improve from 0.98795\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 213.6272 - acc: 0.7249 - val_loss: 32.1904 - val_acc: 0.8434 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 13.3496 - acc: 0.8764\n",
      "Epoch 18: val_acc improved from 0.98795 to 1.00000, saving model to models/model.h5\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 13.3134 - acc: 0.8767 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 13.8572 - acc: 0.9511\n",
      "Epoch 19: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 13.8196 - acc: 0.9512 - val_loss: 5.5481e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 5.6457 - acc: 0.9823\n",
      "Epoch 20: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 5.6304 - acc: 0.9824 - val_loss: 1.8756e-06 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 4.6443 - acc: 0.9810\n",
      "Epoch 21: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 4.6317 - acc: 0.9810 - val_loss: 0.0298 - val_acc: 0.9880 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 4.8636 - acc: 0.9755\n",
      "Epoch 22: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 4.8504 - acc: 0.9756 - val_loss: 0.2845 - val_acc: 0.9880 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 1.8708 - acc: 0.9823\n",
      "Epoch 23: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 1.8658 - acc: 0.9824 - val_loss: 0.2113 - val_acc: 0.9880 - lr: 0.0010\n",
      "Epoch 24/1000\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 1.0365 - acc: 0.9742\n",
      "Epoch 24: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 1.0337 - acc: 0.9743 - val_loss: 1.4422 - val_acc: 0.9759 - lr: 0.0010\n",
      "Epoch 25/1000\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.4541 - acc: 0.9796\n",
      "Epoch 25: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 0.4529 - acc: 0.9797 - val_loss: 0.4333 - val_acc: 0.9759 - lr: 0.0010\n",
      "Epoch 26/1000\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.4183 - acc: 0.9918\n",
      "Epoch 26: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 0.4172 - acc: 0.9919 - val_loss: 0.0681 - val_acc: 0.9880 - lr: 0.0010\n",
      "Epoch 27/1000\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.3210 - acc: 0.9891\n",
      "Epoch 27: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 0.3201 - acc: 0.9892 - val_loss: 3.2469e-06 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 28/1000\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.5750 - acc: 0.9783\n",
      "Epoch 28: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 0.5734 - acc: 0.9783 - val_loss: 0.7116 - val_acc: 0.9759 - lr: 0.0010\n",
      "Epoch 29/1000\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 1.0097 - acc: 0.9674\n",
      "Epoch 29: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 1.0069 - acc: 0.9675 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 30/1000\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.0786 - acc: 0.9959\n",
      "Epoch 30: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 0.0784 - acc: 0.9959 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 31/1000\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.0696 - acc: 0.9946\n",
      "Epoch 31: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 0.0694 - acc: 0.9946 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 32/1000\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.0298 - acc: 0.9973\n",
      "Epoch 32: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 0.0297 - acc: 0.9973 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 33/1000\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.0284 - acc: 0.9973\n",
      "Epoch 33: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 0.0284 - acc: 0.9973 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 34/1000\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.0127 - acc: 0.9973\n",
      "Epoch 34: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 0.0126 - acc: 0.9973 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 35/1000\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.0103 - acc: 0.9986\n",
      "Epoch 35: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 0.0102 - acc: 0.9986 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 36/1000\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.0057 - acc: 0.9986   \n",
      "Epoch 36: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 0.0057 - acc: 0.9986 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 37/1000\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.0048 - acc: 0.9986\n",
      "Epoch 37: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 0.0048 - acc: 0.9986 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 38/1000\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 4.0551e-04 - acc: 1.0000\n",
      "Epoch 38: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 4.0441e-04 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 39/1000\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 3.9319e-04 - acc: 1.0000\n",
      "Epoch 39: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 3.9213e-04 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 40/1000\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 2.1107e-04 - acc: 1.0000\n",
      "Epoch 40: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 2.1049e-04 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 41/1000\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 1.2432e-04 - acc: 1.0000\n",
      "Epoch 41: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 1.2398e-04 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 42/1000\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 1.1801e-04 - acc: 1.0000\n",
      "Epoch 42: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 1s 28ms/step - loss: 1.1257e-04 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 43/1000\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 1.0323e-04 - acc: 1.0000\n",
      "Epoch 43: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 1.0295e-04 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 44/1000\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 1.0047e-04 - acc: 1.0000\n",
      "Epoch 44: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 1.0019e-04 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 45/1000\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 8.4635e-05 - acc: 1.0000\n",
      "Epoch 45: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 8.4405e-05 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 46/1000\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 7.2745e-05 - acc: 1.0000\n",
      "Epoch 46: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 7.2548e-05 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 47/1000\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 6.9864e-05 - acc: 1.0000\n",
      "Epoch 47: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 6.9674e-05 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 48/1000\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 6.8681e-05 - acc: 1.0000\n",
      "Epoch 48: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 6.8494e-05 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 49/1000\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 6.0875e-05 - acc: 1.0000\n",
      "Epoch 49: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 6.0710e-05 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 50/1000\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 6.0013e-05 - acc: 1.0000\n",
      "Epoch 50: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 5.9850e-05 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 51/1000\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 6.5101e-05 - acc: 1.0000\n",
      "Epoch 51: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 6.4924e-05 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 52/1000\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 5.0170e-05 - acc: 1.0000\n",
      "Epoch 52: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 5.0034e-05 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 53/1000\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 5.3441e-05 - acc: 1.0000\n",
      "Epoch 53: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 5.3296e-05 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 54/1000\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 4.9120e-05 - acc: 1.0000\n",
      "Epoch 54: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 4.8987e-05 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 55/1000\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 5.0575e-05 - acc: 1.0000\n",
      "Epoch 55: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 5.0438e-05 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=1000,\n",
    "    callbacks=[\n",
    "        ModelCheckpoint('models/model.h5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto'),\n",
    "        ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=50, verbose=1, mode='auto')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m fig, loss_ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m      4\u001b[0m acc_ax \u001b[38;5;241m=\u001b[39m loss_ax\u001b[38;5;241m.\u001b[39mtwinx()\n\u001b[0;32m----> 6\u001b[0m loss_ax\u001b[38;5;241m.\u001b[39mplot(\u001b[43mhistory\u001b[49m\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m loss_ax\u001b[38;5;241m.\u001b[39mplot(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m loss_ax\u001b[38;5;241m.\u001b[39mset_xlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots(figsize=(16, 10))\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(history.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(history.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "loss_ax.legend(loc='upper left')\n",
    "\n",
    "acc_ax.plot(history.history['acc'], 'b', label='train acc')\n",
    "acc_ax.plot(history.history['val_acc'], 'g', label='val acc')\n",
    "acc_ax.set_ylabel('accuracy')\n",
    "acc_ax.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 9ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[42,  2],\n",
       "        [11, 29]],\n",
       "\n",
       "       [[29, 11],\n",
       "        [ 2, 42]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('models/model.h5')\n",
    "\n",
    "y_pred = model.predict(x_val)\n",
    "\n",
    "multilabel_confusion_matrix(np.argmax(y_val, axis=1), np.argmax(y_pred, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:opencv_keras]",
   "language": "python",
   "name": "conda-env-opencv_keras-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
